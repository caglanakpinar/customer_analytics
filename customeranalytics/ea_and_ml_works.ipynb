{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>.container { width:90% !important; }</style>'))\n",
    "from elasticsearch import Elasticsearch \n",
    "from elasticsearch import helpers\n",
    "from sklearn.cluster import KMeans\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from time import gmtime, strftime\n",
    "from itertools import product\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import chart_studio.plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from os.path import dirname\n",
    "from os import listdir\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from pandas import read_sql\n",
    "from os.path import join, dirname, abspath, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es=Elasticsearch([{'host':'localhost','port':9200}], timeout=300, max_retries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.delete(index='reports', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exploratory_analysis.__init__ import create_exploratory_analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_process.__init__ import create_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.now().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ea_configs = {\"date\": None,\n",
    "              \"funnel\": {\"actions\": [],\n",
    "                         \"purchase_actions\": [\"has_basket\", \"order_screen\"],\n",
    "                         \"host\": 'localhost',\n",
    "                         \"port\": '9200',\n",
    "                         'download_index': 'location_1',\n",
    "                         'order_index': 'location_1'},\n",
    "              \"cohort\": {\"has_download\": True, \"host\": 'localhost', \"port\": '9200',\n",
    "                         'download_index': 'location_1', 'order_index': 'location_1'},\n",
    "              \"products\": {\"has_product_connection\": True, \"host\": 'localhost', \"port\": '9200', 'download_index': 'downloads', 'order_index': 'orders'},\n",
    "              \"rfm\": {\"host\": 'localhost', \"port\": '9200', 'download_index': 'downloads', 'order_index': 'location_2'},\n",
    "              \"stats\": {\"host\": 'localhost', \"port\": '9200', 'download_index': 'downloads', 'order_index': 'location_3'}\n",
    "             }\n",
    "create_exploratory_analyse(ea_configs, 'stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_exploratory_analysis(ea_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.search(index='orders', body={'size': 100, 'from': 0})['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {'size': 10, 'from': 0, '_source': True, 'fields': ['id', 'session_start_date', 'basket'], 'query': {'bool': {'must': [{'term': {'actions.purchased': True}}]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es.search(index='orders', body=match)['hits']['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****  CLV Prediction  *****\n",
      "db connection is done!\n",
      "received : {'job': 'train_prediction', 'order_count': None, 'customer_indicator': 'client', 'amount_indicator': 'payment_amount', 'data_source': 'csv', 'data_query_path': '/Users/caglanakpinar/Desktop/temp_customer_analytics_folder_2/temp_data.csv', 'time_period': '6*month', 'time_indicator': 'session_start_date', 'export_path': '/Users/caglanakpinar/Desktop/temp_customer_analytics_folder_2/'}\n",
      "    session_start_date       client  payment_amount   dimension\n",
      "0  2021-03-31 17:21:55  u_u_1828393          32.750  location_1\n",
      "1  2021-03-31 15:09:13  u_u_1429801          13.630  location_1\n",
      "2  2021-03-31 09:59:40  u_u_1872775          36.970  location_1\n",
      "3  2021-03-31 14:22:51  u_u_1662030          54.720  location_1\n",
      "4  2021-03-31 13:59:47  u_u_1580295           9.935  location_1\n",
      "data size : 359317\n",
      "previous last model trained : 20210613 , for  trained_next_purchase_model\n",
      "***** Next purchase train model process  *****\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Previous model already exits in the given directory  '/Users/caglanakpinar/Desktop/temp_customer_analytics_folder_2/'.\n",
      "***** PREDICTION *****\n",
      "number of users : 57515\n",
      "main iteration : 0  /  272\n",
      "main iteration : 1  /  272\n",
      "main iteration : 2  /  272\n",
      "main iteration : 3  /  272\n",
      "main iteration : 4  /  272\n",
      "main iteration : 5  /  272\n",
      "main iteration : 6  /  272\n",
      "main iteration : 7  /  272\n",
      "main iteration : 8  /  272\n",
      "done!\n",
      "main iteration : 9  /  272\n",
      "main iteration : 10  /  272\n",
      "main iteration : 11  /  272\n",
      "main iteration : 12  /  272\n",
      "main iteration : 13  /  272\n",
      "main iteration : 14  /  272\n",
      "main iteration : 15  /  272\n",
      "main iteration : 16  /  272\n",
      "done!\n",
      "main iteration : 17  /  272\n",
      "main iteration : 18  /  272\n",
      "main iteration : 19  /  272\n",
      "main iteration : 20  /  272\n",
      "main iteration : 21  /  272\n",
      "main iteration : 22  /  272\n",
      "main iteration : 23  /  272\n",
      "main iteration : 24  /  272\n",
      "done!\n",
      "main iteration : 25  /  272\n",
      "main iteration : 26  /  272\n",
      "main iteration : 27  /  272\n",
      "main iteration : 28  /  272\n",
      "main iteration : 29  /  272\n",
      "main iteration : 30  /  272\n",
      "main iteration : 31  /  272\n",
      "main iteration : 32  /  272\n",
      "done!\n",
      "main iteration : 33  /  272\n",
      "main iteration : 34  /  272\n",
      "main iteration : 35  /  272\n",
      "main iteration : 36  /  272\n",
      "main iteration : 37  /  272\n",
      "main iteration : 38  /  272\n",
      "main iteration : 39  /  272\n",
      "main iteration : 40  /  272\n",
      "done!\n",
      "main iteration : 41  /  272\n",
      "main iteration : 42  /  272\n",
      "main iteration : 43  /  272\n",
      "main iteration : 44  /  272\n",
      "main iteration : 45  /  272\n",
      "main iteration : 46  /  272\n",
      "main iteration : 47  /  272\n",
      "main iteration : 48  /  272\n",
      "done!\n",
      "main iteration : 49  /  272\n",
      "main iteration : 50  /  272\n",
      "main iteration : 51  /  272\n",
      "main iteration : 52  /  272\n",
      "main iteration : 53  /  272\n",
      "main iteration : 54  /  272\n",
      "main iteration : 55  /  272\n",
      "main iteration : 56  /  272\n",
      "done!\n",
      "main iteration : 57  /  272\n",
      "main iteration : 58  /  272\n",
      "main iteration : 59  /  272\n",
      "main iteration : 60  /  272\n",
      "main iteration : 61  /  272\n",
      "main iteration : 62  /  272\n",
      "main iteration : 63  /  272\n",
      "main iteration : 64  /  272\n",
      "done!\n",
      "main iteration : 65  /  272\n",
      "main iteration : 66  /  272\n",
      "main iteration : 67  /  272\n",
      "main iteration : 68  /  272\n",
      "main iteration : 69  /  272\n",
      "main iteration : 70  /  272\n",
      "main iteration : 71  /  272\n",
      "main iteration : 72  /  272\n",
      "done!\n",
      "main iteration : 73  /  272\n",
      "main iteration : 74  /  272\n",
      "main iteration : 75  /  272\n",
      "main iteration : 76  /  272\n",
      "main iteration : 77  /  272\n",
      "main iteration : 78  /  272\n",
      "main iteration : 79  /  272\n",
      "main iteration : 80  /  272\n",
      "done!\n",
      "main iteration : 81  /  272\n",
      "main iteration : 82  /  272\n",
      "main iteration : 83  /  272\n",
      "main iteration : 84  /  272\n",
      "main iteration : 85  /  272\n",
      "main iteration : 86  /  272\n",
      "main iteration : 87  /  272\n",
      "main iteration : 88  /  272\n",
      "done!\n",
      "main iteration : 89  /  272\n",
      "main iteration : 90  /  272\n",
      "main iteration : 91  /  272\n",
      "main iteration : 92  /  272\n",
      "main iteration : 93  /  272\n",
      "main iteration : 94  /  272\n",
      "main iteration : 95  /  272\n",
      "main iteration : 96  /  272\n",
      "done!\n",
      "main iteration : 97  /  272\n",
      "main iteration : 98  /  272\n",
      "main iteration : 99  /  272\n",
      "main iteration : 100  /  272\n",
      "main iteration : 101  /  272\n",
      "main iteration : 102  /  272\n",
      "main iteration : 103  /  272\n",
      "main iteration : 104  /  272\n",
      "done!\n",
      "main iteration : 105  /  272\n",
      "main iteration : 106  /  272\n",
      "main iteration : 107  /  272\n",
      "main iteration : 108  /  272\n",
      "main iteration : 109  /  272\n",
      "main iteration : 110  /  272\n",
      "main iteration : 111  /  272\n",
      "main iteration : 112  /  272\n",
      "done!\n",
      "main iteration : 113  /  272\n",
      "main iteration : 114  /  272\n",
      "main iteration : 115  /  272\n",
      "main iteration : 116  /  272\n",
      "main iteration : 117  /  272\n",
      "main iteration : 118  /  272\n",
      "main iteration : 119  /  272\n",
      "main iteration : 120  /  272\n",
      "done!\n",
      "main iteration : 121  /  272\n",
      "main iteration : 122  /  272\n",
      "main iteration : 123  /  272\n",
      "main iteration : 124  /  272\n",
      "main iteration : 125  /  272\n",
      "main iteration : 126  /  272\n",
      "main iteration : 127  /  272\n",
      "main iteration : 128  /  272\n",
      "done!\n",
      "main iteration : 129  /  272\n",
      "main iteration : 130  /  272\n",
      "main iteration : 131  /  272\n",
      "main iteration : 132  /  272\n",
      "main iteration : 133  /  272\n",
      "main iteration : 134  /  272\n",
      "main iteration : 135  /  272\n",
      "main iteration : 136  /  272\n",
      "done!\n",
      "main iteration : 137  /  272\n",
      "main iteration : 138  /  272\n",
      "main iteration : 139  /  272\n",
      "main iteration : 140  /  272\n",
      "main iteration : 141  /  272\n",
      "main iteration : 142  /  272\n",
      "main iteration : 143  /  272\n",
      "main iteration : 144  /  272\n",
      "done!\n",
      "main iteration : 145  /  272\n",
      "main iteration : 146  /  272\n",
      "main iteration : 147  /  272\n",
      "main iteration : 148  /  272\n",
      "main iteration : 149  /  272\n",
      "main iteration : 150  /  272\n",
      "main iteration : 151  /  272\n",
      "main iteration : 152  /  272\n",
      "done!\n",
      "main iteration : 153  /  272\n",
      "main iteration : 154  /  272\n",
      "main iteration : 155  /  272\n",
      "main iteration : 156  /  272\n",
      "main iteration : 157  /  272\n",
      "main iteration : 158  /  272\n",
      "main iteration : 159  /  272\n",
      "main iteration : 160  /  272\n",
      "done!\n",
      "main iteration : 161  /  272\n",
      "main iteration : 162  /  272\n",
      "main iteration : 163  /  272\n",
      "main iteration : 164  /  272\n",
      "main iteration : 165  /  272\n",
      "main iteration : 166  /  272\n",
      "main iteration : 167  /  272\n",
      "main iteration : 168  /  272\n",
      "done!\n",
      "main iteration : 169  /  272\n",
      "main iteration : 170  /  272\n",
      "main iteration : 171  /  272\n",
      "main iteration : 172  /  272\n",
      "main iteration : 173  /  272\n",
      "main iteration : 174  /  272\n",
      "main iteration : 175  /  272\n",
      "main iteration : 176  /  272\n",
      "done!\n",
      "main iteration : 177  /  272\n",
      "main iteration : 178  /  272\n",
      "main iteration : 179  /  272\n",
      "main iteration : 180  /  272\n",
      "main iteration : 181  /  272\n",
      "main iteration : 182  /  272\n",
      "main iteration : 183  /  272\n",
      "main iteration : 184  /  272\n",
      "done!\n",
      "main iteration : 185  /  272\n",
      "main iteration : 186  /  272\n",
      "main iteration : 187  /  272\n",
      "main iteration : 188  /  272\n",
      "main iteration : 189  /  272\n",
      "main iteration : 190  /  272\n",
      "main iteration : 191  /  272\n",
      "main iteration : 192  /  272\n",
      "done!\n",
      "main iteration : 193  /  272\n",
      "main iteration : 194  /  272\n",
      "main iteration : 195  /  272\n",
      "main iteration : 196  /  272\n",
      "main iteration : 197  /  272\n",
      "main iteration : 198  /  272\n",
      "main iteration : 199  /  272\n",
      "main iteration : 200  /  272\n",
      "done!\n",
      "main iteration : 201  /  272\n",
      "main iteration : 202  /  272\n",
      "main iteration : 203  /  272\n",
      "main iteration : 204  /  272\n",
      "main iteration : 205  /  272\n",
      "main iteration : 206  /  272\n",
      "main iteration : 207  /  272\n",
      "main iteration : 208  /  272\n",
      "done!\n",
      "main iteration : 209  /  272\n",
      "main iteration : 210  /  272\n",
      "main iteration : 211  /  272\n",
      "main iteration : 212  /  272\n",
      "main iteration : 213  /  272\n",
      "main iteration : 214  /  272\n",
      "main iteration : 215  /  272\n",
      "main iteration : 216  /  272\n"
     ]
    }
   ],
   "source": [
    "ml_configs = {\"date\": None,\n",
    "              'time_period': '6 months',\n",
    "              \"segmentation\": {\"host\": 'localhost', \"port\": '9200',\n",
    "                               'download_index': 'downloads', 'order_index': 'orders'},\n",
    "              \"clv_prediction\": {\"temporary_export_path\": \"/Users/caglanakpinar/Desktop/temp_customer_analytics_folder_2/\",\n",
    "                                 \"host\": 'localhost', \"port\": '9200',\n",
    "                                 'download_index': 'downloads', 'order_index': 'orders', 'time_period': '6 months'},\n",
    "              \"abtest\": {\"has_product_connection\": False,\n",
    "                         \"has_promotion_connection\": False, \"temporary_export_path\": \"/Users/caglanakpinar/Desktop/temp_customer_analytics_folder/\",\n",
    "                         \"host\": 'localhost', \"port\": '9200', 'download_index': 'downloads', 'order_index': 'location_3'},\n",
    "              \"anomaly\": {\"host\": 'localhost', \"port\": '9200',\n",
    "                          'download_index': 'downloads', 'order_index': 'orders'},\n",
    "          }\n",
    "create_ml(ml_configs, 'clv_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"/Users/mac/Downloads/es-7.10.0/results_week_20210603_20210529.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([i for i in range(100000)])\n",
    "sum(data.memory_usage()) / 8000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {'size': 1, 'from': 0, '_source': True, 'fields': ['id', 'client', 'basket', 'session_start_date', 'payment_amount'], 'query': {'bool': {'must': [{'term': {'actions.has_basket': True}}, {'range': {'session_start_date': {'lt': '2021-04-03'}}}]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.search(index='orders', body=match)['hits']['hits'][0]['_source']['basket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "res = []\n",
    "for r in es.search(index='reports', body={\"size\": 1000, \"from\": 0})['hits']['hits']:\n",
    "    try:\n",
    "        print(r['_source'].keys())\n",
    "        res.append({'report_name': r['_source']['report_name'],\n",
    "                    'report_date': r['_source']['report_date'],\n",
    "                    'time_period': r['_source']['report_types'].get('time_period', None),\n",
    "                    'type': r['_source']['report_types'].get('type', None),\n",
    "                    '_from': r['_source']['report_types'].get('from', None),\n",
    "                    '_to': r['_source']['report_types'].get('to', None),\n",
    "                    'abtest_type': r['_source']['report_types'].get('abtest_type', None),\n",
    "                    'report_types': r['_source']['report_types'],\n",
    "                    'index': r['_source']['index'],\n",
    "                    'data': r['_source']['data']})\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pd.DataFrame(res).query(\"report_name == 'clv_prediction'\")['data'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'clv_prediction'\")['data'])[-1]).query(\"client  == 'newcomers'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res)['report_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"report_name == 'anomaly'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'anomaly' and type == 'daily_funnel'\")['data'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'cohort' and time_period == 'daily' and type == 'downloads'\")['data'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'funnel' and time_period == 'daily' and type == 'downloads'\"\n",
    "                                         )['data'])[0]).to_csv(\"exploratory_analysis/sample_data/sample_data_daily_funnel_downloads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'funnel' and time_period == 'hourly' and type == 'downloads'\"\n",
    "                                         )['data'])[0]).to_csv(\"exploratory_analysis/sample_data/sample_data_hourly_funnel_downloads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'cohort' and time_period == 'daily' and type == 'orders' and to == 1\")['data'])[0]).to_csv(\n",
    "    \"exploratory_analysis/sample_data/sample_data_weekly_cohort_downloads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'cohort' and time_period == 'daily' and type == 'orders' and to == '2.0'\")['data'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'cohort' and time_period == 'weekly' and type == 'downloads' and to == '1.0'\")['data'])[0]).to_csv(\n",
    "    \"exploratory_analysis/sample_data/sample_data_weekly_cohort_downloads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['daily_cohort_downloads', 'daily_cohort_from_1_to_2', 'daily_cohort_from_2_to_3', 'daily_cohort_from_3_to_4',\n",
    "                              'weekly_cohort_downloads', 'weekly_cohort_from_1_to_2', 'weekly_cohort_from_2_to_3', 'weekly_cohort_from_3_to_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"report_name == 'stats'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_date(date):\n",
    "    \"\"\"\n",
    "    Convert str date to datetime. If is NULL skip the process and return None.\n",
    "    :param date: str format; %Y-%m-%d %H:%M:%S\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if date == date:\n",
    "        return datetime.datetime.strptime(str(date)[0:10] + ' ' + str(date)[11:19], \"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {\n",
    "   \"size\": 1,\n",
    "   \"sort\": { \"session_start_date\": \"desc\"},\n",
    "\n",
    "}\n",
    "convert_to_date(es.search(index='orders', body={\"size\": 1, \"sort\": { \"session_start_date\": \"desc\"}})['hits']['hits'][0]['_source']['session_start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match2 = {\n",
    "\"size\": 0,\n",
    "\"aggs\" : {\n",
    "    \"langs\" : {\n",
    "        \"terms\" : { \"field\" : \"dimension\",  \"size\" : 500 }\n",
    "    }\n",
    "}}\n",
    "\n",
    "es.search(index='orders', body=match2)['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r['fields']['dimension'][0] for r in es.search(index='orders', body={\"fields\": [\"dimension\"]})['hits']['hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_hourly_orders.csv\")).drop('Unnamed: 0', axis=1).rename(columns={\"hours\": \"hourly\"})\n",
    "data.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_hourly_orders.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_weekly_orders.csv\")).drop('Unnamed: 0', axis=1).rename(columns={\"date\": \"weekly\"})\n",
    "data.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_weekly_orders.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_monthly_orders.csv\")).drop('Unnamed: 0', axis=1).rename(columns={\"date\": \"monthly\"})\n",
    "data.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_monthly_orders.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_daily_orders.csv\")).drop('Unnamed: 0', axis=1).rename(columns={\"date\": \"daily\"})\n",
    "data.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"sample_data_daily_orders.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_average_session_per_user = pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'stats' and type == 'weekly_average_session_per_user'\")['data'])[-1])\n",
    "weekly_average_order_per_user = pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'stats' and type == 'weekly_average_order_per_user'\")['data'])[-1])\n",
    "purchase_amount_distribution = pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'stats' and type == 'purchase_amount_distribution'\")['data'])[-1])\n",
    "weekly_average_payment_amount = pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'stats' and type == 'weekly_average_payment_amount'\")['data'])[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = zip([\"weekly_average_session_per_user\", \"weekly_average_order_per_user\", \"purchase_amount_distribution\", \"weekly_average_payment_amount\"], \n",
    "          [weekly_average_session_per_user, weekly_average_order_per_user, purchase_amount_distribution, weekly_average_payment_amount])\n",
    "for df in dfs:\n",
    "    df[1].to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", df[0] + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_amount_distribution['payment_bins'] = purchase_amount_distribution['payment_bins'].apply(lambda x: float(x))\n",
    "purchase_amount_distribution['orders'] = purchase_amount_distribution['orders'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_amount_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"report_name == 'stats'\")['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_amount_distribution['orders'] = purchase_amount_distribution['orders'].apply(lambda x: int(x))\n",
    "purchase_amount_distribution = purchase_amount_distribution.sort_values(by='payment_bins', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "purchase_amount_distribution['payment_bins'] = purchase_amount_distribution['payment_bins'].apply(lambda x: float(x))\n",
    "purchase_amount_distribution['orders'] = purchase_amount_distribution['orders'].apply(lambda x: int(x))\n",
    "purchase_amount_distribution = purchase_amount_distribution.sort_values(by='payment_bins', ascending=True)\n",
    "trace = []\n",
    "for _bin in purchase_amount_distribution.to_dict('results'):\n",
    "    print(_bin['payment_bins'], _bin['orders'])\n",
    "    trace.append(go.Bar(x=[_bin['payment_bins']], y=[_bin['orders']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "animals=['giraffes', 'orangutans', 'monkeys']\n",
    "\n",
    "fig = go.Figure(data=trace)\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='group', color='blue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dict(i)['abtest_type'] for i in list(pd.DataFrame(res).query(\"report_name == 'abtest'\")['report_types'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abtests = pd.DataFrame(res).query(\"report_name == 'abtest'\")\n",
    "abtests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test_values = {'promotion_usage_before_after_orders': pd.DataFrame(),\n",
    "                  'product_usage_before_after_amount': pd.DataFrame(),\n",
    "                  'segments_change_weekly_before_after_orders': pd.DataFrame(),\n",
    "                  'segments_change_monthly_before_after_orders': pd.DataFrame(),\n",
    "                  'promotion_usage_before_after_amount': pd.DataFrame(),\n",
    "                  'product_usage_before_after_orders': pd.DataFrame(),\n",
    "                  'segments_change_daily_before_after_orders': pd.DataFrame(),\n",
    "                  'segments_change_daily_before_after_amount': pd.DataFrame(),\n",
    "                  'segments_change_weekly_before_after_amount': pd.DataFrame(),\n",
    "                  'segments_change_monthly_before_after_amount': pd.DataFrame(),\n",
    "                  'promotion_comparison': pd.DataFrame()}\n",
    "\n",
    "for i in pd.DataFrame(res).query(\"report_name == 'abtest'\").to_dict('results'):\n",
    "    ab_test_values[i['report_types']['abtest_type']] = pd.DataFrame(i['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test_values['promotion_usage_before_after_orders']['diff'] = ab_test_values['promotion_usage_before_after_orders']['mean_validation'] - ab_test_values[\n",
    "    'promotion_usage_before_after_orders']['mean_control']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before After Customers of Average Purchase Count Test (Test Rejected!)\n",
    "    - Assumption \"Promotion of usage increase the purchase count of each user?\"\n",
    "    - This graph shows the promotion which we tested and rejected of the Assumption.\n",
    "    - Related the customers who has used promotions, This is the A/B Test reults of these customers of before order count of each customer and after order count of each result.\n",
    "    - Graph shows the customers of the average purchase count of  of before and after the usage of each promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_abtest = ab_test_values['promotion_usage_before_after_orders'].query(\"is_orders_increased_per_promotions == False\").sort_values('diff')\n",
    "trace = [go.Bar(name=\"before avereage order count per customers\", x=_abtest['promotions'], y=_abtest['mean_control'], text=_abtest['diff']),\n",
    "         go.Bar(name=\"after avereage order count per customers\", x=_abtest['promotions'], y=_abtest['mean_validation'], text=_abtest['diff'])\n",
    "         ]\n",
    "fig = go.Figure(data=trace, layout_yaxis_range=[round(max(0, min(min(_abtest['mean_control']), min(_abtest['mean_validation'])) - 0.02), 2), \n",
    "                                               round(max(0, max(max(_abtest['mean_control']), max(_abtest['mean_validation'])) - 0.02), 2)])\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, )\n",
    "# Change the bar mode\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_abtest.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"promotion_usage_before_after_orders_accept.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before After Customers of Average Purchase Count Test (Test Accepted!)\n",
    "    - Assumption \"Promotion of usage increase the purchase count of each user?\"\n",
    "    - This graph shows the promotion which we tested and Accepted of the Assumption.\n",
    "    - Related the customers who has used promotions, This is the A/B Test reults of these customers of before order count of each customer and after order count of each result.\n",
    "    - Graph shows the customers of the average purchase count of  of before and after the usage of each promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_abtest = ab_test_values['promotion_usage_before_after_orders'].query(\"is_orders_increased_per_promotions == True\")\n",
    "trace = [go.Bar(name=\"before avereage order count per customers\", x=_abtest['promotions'], y=_abtest['mean_control']),\n",
    "         go.Bar(name=\"after avereage order count per customers\", x=_abtest['promotions'], y=_abtest['mean_validation'])\n",
    "         ]\n",
    "fig = go.Figure(data=trace, layout_yaxis_range=[round(max(0, min(min(_abtest['mean_control']), min(_abtest['mean_validation'])) - 0.02), 2), \n",
    "                                               round(max(0, max(max(_abtest['mean_control']), max(_abtest['mean_validation'])) - 0.02), 2)])\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45)\n",
    "\n",
    "# Change the bar mode\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_abtest.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"promotion_usage_before_after_orders_reject.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before After Customers of Average Purchase Amount Test (Test Rejected!)\n",
    "    - Assumption \"Promotion of usage increase the purchase count of each user?\"\n",
    "    - This graph shows the promotion which we tested and rejected of the Assumption.\n",
    "    - Related the customers who has used promotions, This is the A/B Test reults of these customers of before order count of each customer and after order count of each result.\n",
    "    - Graph shows the customers of the average purchase count of  of before and after the usage of each promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test_values['promotion_usage_before_after_amount']['diff'] = ab_test_values['promotion_usage_before_after_amount']['mean_validation'] - ab_test_values[\n",
    "    'promotion_usage_before_after_amount']['mean_control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_abtest = ab_test_values['promotion_usage_before_after_amount'].query(\"is_amount_increased_per_promotions == False\").sort_values('diff')\n",
    "trace = [go.Bar(name=\"before avereage purchase amount per customers\", x=_abtest['promotions'], y=_abtest['mean_control'], text=_abtest['diff']),\n",
    "         go.Bar(name=\"after avereage purchase amount per customers\", x=_abtest['promotions'], y=_abtest['mean_validation'], text=_abtest['diff'])\n",
    "         ]\n",
    "fig = go.Figure(data=trace, layout_yaxis_range=[round(max(0, min(min(_abtest['mean_control']), min(_abtest['mean_validation'])) - 0.02), 2), \n",
    "                                               round(max(0, max(max(_abtest['mean_control']), max(_abtest['mean_validation'])) - 0.02), 2)])\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, )\n",
    "# Change the bar mode\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_abtest.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"promotion_usage_before_after_amount_reject.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before After Customers of Average Purchase Count Test (Test Accepted!)\n",
    "    - Assumption \"Promotion of usage increase the purchase count of each user?\"\n",
    "    - This graph shows the promotion which we tested and Accepted of the Assumption.\n",
    "    - Related the customers who has used promotions, This is the A/B Test reults of these customers of before order count of each customer and after order count of each result.\n",
    "    - Graph shows the customers of the average purchase count of  of before and after the usage of each promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_abtest = ab_test_values['promotion_usage_before_after_amount'].query(\"is_amount_increased_per_promotions == True\")\n",
    "trace = [go.Bar(name=\"before avereage order count per customers\", x=_abtest['promotions'], y=_abtest['mean_control']),\n",
    "         go.Bar(name=\"after avereage order count per customers\", x=_abtest['promotions'], y=_abtest['mean_validation'])\n",
    "         ]\n",
    "fig = go.Figure(data=trace, layout_yaxis_range=[round(max(0, min(min(_abtest['mean_control']), min(_abtest['mean_validation'])) - 0.02), 2), \n",
    "                                               round(max(0, max(max(_abtest['mean_control']), max(_abtest['mean_validation'])) - 0.02), 2)])\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45)\n",
    "\n",
    "# Change the bar mode\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_abtest.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"promotion_usage_before_after_amount_accept.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promotions of Before After Order Count and Payment Amount Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_amount_ab = pd.merge(ab_test_values['promotion_usage_before_after_orders'], \n",
    "         ab_test_values['promotion_usage_before_after_amount'].rename(columns={'diff': 'diff_amount'}), on='promotions', how='inner')[['diff_amount', 'diff', 'promotions']]\n",
    "order_amount_ab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(order_amount_ab, x=\"diff\", y=\"diff_amount\", color=\"promotions\",\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_amount_ab.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"order_and_payment_amount_differences.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do promotions work on customers of before - after order count generally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_control_order = np.mean(ab_test_values['promotion_usage_before_after_orders']['mean_control'])\n",
    "mean_validation_order = np.mean(ab_test_values['promotion_usage_before_after_orders']['mean_validation'])\n",
    "mean_control_amount = np.mean(ab_test_values['promotion_usage_before_after_amount']['mean_control'])\n",
    "mean_validation_amount = np.mean(ab_test_values['promotion_usage_before_after_amount']['mean_validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trace = [go.Bar(name=\"before avereage purchase amount per customers\", x=['before', 'after'], y=[mean_control_order]),\n",
    "         go.Bar(name=\"after avereage purchase amount per customers\", x=['after', 'after'], y=[mean_validation_order])\n",
    "         ]\n",
    "fig = go.Figure(data=trace)\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, )\n",
    "# Change the bar mode\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "trace = [go.Bar(name=\"before avereage purchase amount per customers\", x=['before', 'after'], y=[mean_control_order, mean_validation_order]),\n",
    "         go.Bar(name=\"after avereage purchase amount per customers\", x=['before', 'after'], y=[mean_control_amount, mean_validation_amount])\n",
    "         ]\n",
    "fig = go.Figure(data=trace)\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45, )\n",
    "# Change the bar mode\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comaprison of Promotions\n",
    "    - Assumption: \"Orders with 1st promotion have significantly more effects on Payment Amount then orders with 2nd Promotion of Payments Amount.\"\n",
    "    - number of  Promotion Pairs of Comparison: promotion pairs all combinations are tested. The number of 1st Promotion of Accepted tests are counted as 1st promotion has significantly more effects on Payment Amount then 2nd Promotion. \n",
    "    - Accept Ratio: Ratio of Accepted the assumption.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test = ab_test_values['promotion_comparison']\n",
    "_ab_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test.sort_values('accept_Ratio', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test['diff'] = _ab_test['mean_validation'] - _ab_test['mean_control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_promotion_comparison(x):\n",
    "    _x = x.split(\"_\")\n",
    "    return pd.Series([\"_\".join(_x[0:2]), \"_\".join(_x[2:4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test[['1st promo', '2nd Promo']] = _ab_test['promotion_comparison'].apply(lambda x: get_promotion_comparison(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test['total_positive_effects'] = _ab_test['promo_1st_vs_promo_2nd'].apply(lambda x: 1 if x in ['True', True] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test_pv = _ab_test.groupby(\"1st promo\").agg({\"total_effects\": \"sum\", \n",
    "                                                 \"accept_Ratio\": \"mean\", \"promo_1st_vs_promo_2nd\": \"count\"}\n",
    "                                               ).reset_index().sort_values(['total_effects'], ascending=False).sort_values(\n",
    "    ['accept_Ratio'], ascending=True).rename(columns={\"promo_1st_vs_promo_2nd\": \"total_negative_effects\"})\n",
    "_ab_test_pv['total_negative_effects'] = _ab_test_pv['total_negative_effects'] - _ab_test_pv['total_effects']\n",
    "_ab_test_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(_ab_test_pv, x=\"accept_Ratio\", y=\"total_effects\", color=\"1st promo\",\n",
    "                 size='total_negative_effects')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test_pv.to_csv(join(abspath(\"\"), \"exploratory_analysis\", \"sample_data\", \"promotion_comparison.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ab_test_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=_ab_test_pv['accept_Ratio'], \n",
    "                         y=_ab_test_pv['total_effects'], \n",
    "                         text=_ab_test_pv['1st promo'],\n",
    "                         marker=dict(size=_ab_test_pv['total_negative_effects'], color=list(range(len(_ab_test_pv))), colorscale='Rainbow'),\n",
    "                    mode='markers',\n",
    "                    name='markers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'rfm'\")['data'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"report_name == 'product_analytic'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\n",
    "    \"report_name == 'product_analytic' and type == 'most_ordered_products'\"\n",
    ")['data'])[0]).sort_values(by='order_count', ascending=False).iloc[:10].to_csv(\"sample_data_most_ordered_products.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\n",
    "    \"report_name == 'product_analytic' and type == 'most_ordered_products'\"\n",
    ")['data'])[0]).sort_values(by='order_count', ascending=False).iloc[:10]# .to_csv(\"sample_data_most_ordered_categories.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\n",
    "    \"report_name == 'cohort' and type == 'customers_journey' and index == 'main'\")['data'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res)['index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for r in es.search(index='reports', body={\"size\": 1000, \"from\": 0})['hits']['hits']:\n",
    "    try:\n",
    "        res.append({'report_name': r['_source']['report_name'],\n",
    "                    'report_date': r['_source']['report_date'],\n",
    "                    'time_period': r['_source']['report_types'].get('time_period', None),\n",
    "                    'type': r['_source']['report_types'].get('type', None),\n",
    "                    'from': r['_source']['report_types'].get('from', None),\n",
    "                    'to': r['_source']['report_types'].get('to', None),\n",
    "                    'abtest_type': r['_source']['report_types'].get('abtest_type', None),\n",
    "                    'report_types': r['_source']['report_types'],\n",
    "                    'index': r['_source']['index'],\n",
    "                    'data': r['_source']['data']})\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"report_name == 'clv_prediction'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"index == 'location_1'\")['report_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"index == 'main' and report_name == 'stats'\")['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res)['report_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'rfm'\")['data'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_res = es.search(index='orders', body={\"size\": 0, \n",
    "                                                               \"aggs\" : {\n",
    "        \"langs\" : {\n",
    "            \"terms\" : { \"field\" : \"dimension.keyword\",  \n",
    "                        \"size\" : 500 }\n",
    "        }\n",
    "    }\n",
    "                               \n",
    "                               })\n",
    "_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = [r['key'] for r in _res['aggregations']['langs']['buckets']] + ['main']\n",
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_reports(port, host, index, query=None):\n",
    "    res = []\n",
    "\n",
    "    match = {\"size\": 1000, \"from\": 0}\n",
    "    for r in es.search(index='reports', body=match)['hits']['hits']:\n",
    "        if r['_source']['index'] == index:\n",
    "            try:\n",
    "                _obj = {'report_name': r['_source']['report_name'],\n",
    "                        'report_date': r['_source']['report_date'],\n",
    "                        'time_period': r['_source']['report_types'].get('time_period', None),\n",
    "                        'type': r['_source']['report_types'].get('type', None),\n",
    "                        '_from': r['_source']['report_types'].get('from', None),\n",
    "                        '_to': r['_source']['report_types'].get('to', None),\n",
    "                        'abtest_type': r['_source']['report_types'].get('abtest_type', None),\n",
    "                        'report_types': r['_source']['report_types'],\n",
    "                        'index': r['_source']['index']\n",
    "                        }\n",
    "                if query is not None:\n",
    "                    _obj['data'] = r['_source']['data']\n",
    "                res.append(_obj)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_tag = {\"host\": 'localhost', \"port\": 9200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_reports = {'promotion_usage_before_after_amount': ['promotion_usage_before_after_amount_accept',\n",
    "                                                                       'promotion_usage_before_after_amount_reject',\n",
    "                                                                       'order_and_payment_amount_differences'],\n",
    "                               'promotion_usage_before_after_orders': ['promotion_usage_before_after_orders_accept',\n",
    "                                                                       'promotion_usage_before_after_orders_reject']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_name(k, c):\n",
    "    if k['report_name'] == 'funnel':\n",
    "        r_name = k['time_period'] + '_funnel'\n",
    "        if k['type'] == 'downloads':\n",
    "            r_name += '_downloads'\n",
    "    if k['report_name'] == 'cohort':\n",
    "        if k['type'] == 'orders':\n",
    "            r_name = \"_\".join([k['time_period'], 'cohort_from', str(int(k['_from'])), 'to', str(int(k['_to']))])\n",
    "        else:\n",
    "            r_name = \"_\".join([k['time_period'], 'cohort', k['type']])\n",
    "    if k['report_name'] == 'stats':\n",
    "        r_name = k['type'] if k['type'] != '' else 'kpis'\n",
    "    if k['report_name'] == 'abtest':\n",
    "        if k['abtest_type'] in list(double_reports.keys()):\n",
    "            r_name = double_reports[k['abtest_type']]\n",
    "        else:\n",
    "            r_name = k['abtest_type']\n",
    "    if k['report_name'] == 'product_analytic':\n",
    "        r_name = k['type']\n",
    "    if k['report_name'] not in ['cohort', 'funnel', 'stats', 'product_analytic', 'abtest']:\n",
    "        r_name = k['report_name']\n",
    "\n",
    "    return r_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpis = {}\n",
    "for index in dimension:\n",
    "    reports = collect_reports(es_tag['port'], es_tag['host'], index)\n",
    "    for k in reports.to_dict('results'):\n",
    "        r_name = report_name(k, index)\n",
    "        if type(r_name) == list:\n",
    "            for sub_r_name in r_name:\n",
    "                kpis[sub_r_name] = {i: k[i] for i in list(k.keys())}\n",
    "        else:\n",
    "            kpis[r_name] = {i: k[i] for i in list(k.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(kpis.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {'size': 10000000, 'from': 0, '_source': True, 'query': {'bool': {'must': [{'term': {'index': 'main'}}, {'term': {'report_name': 'stats'}}, {'range': {'report_date': {'gt': '2021-04-22T00:00:00', 'lt': '2021-04-23T18:49:18.337762'}}}]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"report_name == 'abtest'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([i['abtest_type'] for i in list(pd.DataFrame(res).query(\"report_name == 'abtest'\")['report_types'])]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(pd.DataFrame(res).query(\"report_name == 'segmentation'\").to_dict('results')[0]['data'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clients = len(data)\n",
    "data = data.groupby(\"segments\").agg({\"client\": \"count\"}).reset_index().rename(columns={\"client\": \"value\"})\n",
    "data['value'] = round((data['value'] * 100) / total_clients, 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'stats' and type == 'daily_orders'\")['data'])[-1]).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\"report_name == 'funnel' and type == 'daily_orders'\").to_dict('results')[-1]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pd.DataFrame(res).query(\"report_name == 'abtest'\")['abtest_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {'size': 10000000, 'from': 0, '_source': True, \n",
    "         'query': {'bool': {'must': [{'term': {'index': 'main'}}, {'term': {'report_name': 'abtest'}}, \n",
    "                                     {'range': {'report_date': {'gt': '2021-04-24T00:00:00', 'lt': '2021-04-25T18:43:02.799896'}}}]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([i['_source']['report_types']['abtest_type'] for  i in es.search(index='reports', body=match)['hits']['hits'] if i['_source']['report_name'] == 'abtest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.search(index='reports', body=match)['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pd.DataFrame(res).query(\"report_name == 'product_analytic'\")['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'product_analytic' and type == 'hourly_categories_of_sales'\")['data'])[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def current_date_to_day():\n",
    "    \"\"\"\n",
    "    recent date of converting to datetime from isoformat.\n",
    "    :return: datetime\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(str(datetime.datetime.now())[0:19], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "def convert_to_date(date):\n",
    "    \"\"\"\n",
    "    Convert str date to datetime. If is NULL skip the process and return None.\n",
    "    :param date: str format; %Y-%m-%d %H:%M:%S\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if date == date:\n",
    "        return datetime.datetime.strptime(str(date)[0:10] + ' ' + str(date)[11:19], \"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/caglanakpinar/Downloads/temporary_folder_for_ca/logs.log') as f:\n",
    "    file = f.readlines()\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'cakpinar23@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {'ds_connect': \"{} Data Source is created or updated\", \n",
    " 'es_connect': \"ElasticSerach Connection is created.\",\n",
    " 'es_delete': \"ElasticSerach Connection is deleted.\",\n",
    " 'schedule_start': \"Scehduling Process is initialized.\",\n",
    " 'schedule_end': \"Scehduling Process is done.\",\n",
    " 'schedule_delete': \"Scehduling Process is deleted.\"}\n",
    "\n",
    "data_types = {'orders': 'Sessions', 'downloads': 'Customers', 'products': 'Products (Baskets)'}\n",
    "def create_notification(user, text):\n",
    "    splits = text.split(\"-\")\n",
    "    if splits[0] == 'ds':\n",
    "        message = messages[ds_connect].format(data_types[splits[1].split(\"_\")[0]])\n",
    "    else:\n",
    "        message = messages[\"_\".join([splits[0], splits[-1]])]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in file:\n",
    "    if l != '\\n':\n",
    "        try:\n",
    "            if convert_to_date(l[0:19]) > current_date_to_day() - datetime.timedelta(days=7):\n",
    "                if user in l:\n",
    "                    print(l)\n",
    "                    print(l.split(user)[-1])\n",
    "        except:\n",
    "            print(\"can not parse logs\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "last_7_days_user_logs = [l for l in file if convert_to_date(l[0:19]) > current_date_to_day() - datetime.timedelta(days=7)]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = \"sasd*dddd*1111*3333\"\n",
    "_target, _lchart, _index, _date = list(plot.split(\"*\"))\n",
    "_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/caglanakpinar/Downloads/temporary_folder_for_ca/logs.log', 'w') as f:\n",
    "    f.write(\"\\n\".join(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_names = {\n",
    "    \"Sessions Of Actions Funnel\":\n",
    "               {\"Daily Funnel\": \"funnel*daily_funnel\", \n",
    "                \"Weekly Funnel\": \"funnel*weekly_funnel\", \n",
    "                \"Monthly Funnel\": \"funnel*monthly_funnel\", \n",
    "                \"Hourly Funnel\": \"funnel*hourly_funnel\"}, \n",
    "    \n",
    "    \"Customers Of Actions Funnel\":\n",
    "               {\"Daily Funnel\": \"funnel*daily_funnel_downloads\", \n",
    "                \"Weekly Funnel\": \"funnel*weekly_funnel_downloads\", \n",
    "                \"Monthly Funnel\": \"funnel*monthly_funnel_downloads\", \n",
    "                \"Hourly Funnel\": \"funnel*hourly_funnel_downloads\"}, \n",
    "    \"Cohorts\":\n",
    "               {\"Daily Download to 1st Order Cohort\": \"cohort*daily_cohort_downloads\", \n",
    "                \"Daily From 1st to 2nd Order Cohort\": \"cohort*daily_cohort_from_1_to_2\", \n",
    "                \"Daily From 2nd to 3rd Order Cohort\": \"cohort*daily_cohort_from_2_to_3\", \n",
    "                \"Daily From 3rd to 4th Order Cohort\": \"cohort*daily_cohort_from_3_to_4\", \n",
    "                \"Weekly Download to 1st Order Cohort\": \"cohort*weekly_cohort_downloads\",\n",
    "                \"Weekly From 1st to 2nd Order Cohort\": \"cohort*weekly_cohort_from_1_to_2\", \n",
    "                \"Weekly From 2nd to 3rd Order Cohort\": \"cohort*weekly_cohort_from_2_to_3\", \n",
    "                \"Weekly From 3rd to 4th Order Cohort\": \"cohort*weekly_cohort_from_3_to_4\"},\n",
    "    \"Descriptive Stats\":\n",
    "               {\"Daily Orders\": \"stats*daily_orders\", \n",
    "                \"Hourly Orders\": \"stats*hourly_orders\", \n",
    "                \"Weekly Orders\": \"stats*weekly_orders\", \n",
    "                \"Monthly Orders\": \"stats*monthly_orders\", \n",
    "                \"Weekly Average Session Count per Customer\": \"descriptive*weekly_average_session_per_user\", \n",
    "                \"Weekly Average Purchase Count per Customer\": \"descriptive*weekly_average_payment_amount\", \n",
    "                \"Payment Amount Distribution\": \"descriptive*purchase_amount_distribution\", \n",
    "                \"Weekly Average Payment Amount\": \"descriptive*weekly_average_payment_amount\"}, \n",
    "    \"Product Analytics\":\n",
    "               {\"Most Combined Products\": \"product_analytic*most_combined_products\", \n",
    "                \"Most Order Products\": \"product_analytic*most_ordered_products\", \n",
    "                \"Most Order Categories\": \"product_analytic*most_ordered_categories\"},     \n",
    "    \"A/B test Promotion\":\n",
    "               {\"Promotion Comparison\": \"abtest-promotion*promotion_comparison\", \n",
    "                \"Order And Payment Amount Difference for Before And After Promotion Usage\": \"abtest-promotion*order_and_payment_amount_differences\", \n",
    "                \"A/B Test Promotion B. - A. Time Periods Cust.s' Avg. Purchase Payment Amount Test (Test Accepted!)\": \"abtest-promotion*promotion_usage_before_after_amount_accept\", \n",
    "                \"A/B Test Promotion B. - A. Time Periods Cust.s' Avg. Purchase Payment Amount Test (Test Rejected!)\": \"abtest-promotion*promotion_usage_before_after_amount_reject\", \n",
    "                \"A/B Test Promotion B. - A. Time Periods Cust.s' Tot. Purchase Count Test (Test Accepted!)\": \"abtest-promotion*promotion_usage_before_after_orders_accept\", \n",
    "                \"A/B Test Promotion B. - A. Time Periods Cust.s' Tott Purchase Count Test (Test Rejected!)\": \"abtest-promotion*promotion_usage_before_after_orders_reject\"}, \n",
    "    \"A/B Test Product\":\n",
    "               {\"A/B Test Product - B. - A. Time Periods Cust.s' Avg. Purchase Payment Amount Test (Test Accepted!)\": \"abtest-product*product_usage_before_after_amount_accept\", \n",
    "                \"A/B Test Product - B. - A. Time Periods Cust.s' Avg. Purchase Payment Amount Test (Test Rejected!)\": \"abtest-product*product_usage_before_after_amount_reject\", \n",
    "                \"A/B Test Product - B. - A. Time Periods Cust.s' Totg Purchase Count Test (Test Accepted!)\": \"abtest-product*product_usage_before_after_orders_accept\", \n",
    "                \"A/B Test Product - B. - A. Time Periods Cust.s' Totg Purchase Count Test (Test Rejected!)\": \"abtest-product*product_usage_before_after_orders_reject\"},\n",
    "    \"A/B Test Customer Segment Change\":\n",
    "               {\"A/B Test Cust. Segment Change - A/B Test Product - Daily Customers' Total Order Count per Customer Segment\": \"abtest-segments*segments_change_daily_before_after_orders\", \n",
    "                \"Weekly Customers' Total Order Count per Customer Segment\": \"abtest-segments*segments_change_weekly_before_after_orders\", \n",
    "                \"Monthly Customers' Total Order Count per Customer Segment\": \"abtest-segments*segments_change_weekly_before_after_orders\", \n",
    "                \"Daily Customers' Average Purchase Payment Amount per Customer Segment\": \"abtest-segments*segments_change_daily_before_after_amount\", \n",
    "                \"Weekly Customers' Average Purchase Payment Amount per Customer Segment\": \"abtest-segments*segments_change_weekly_before_after_amount\", \n",
    "                \"Monthly Customers' Average Purchase Payment Amount per Customer Segment\": \"abtest-segments*segments_change_weekly_before_after_amount\"},       \n",
    "    \"RFM\":\n",
    "               {\"RFM\": \"rfm*rfm\", \n",
    "                \"Frequency - Recency\": \"rfm*frequency_recency\", \n",
    "                \"Monetary - Frequency\": \"rfm*monetary_frequency\", \n",
    "                \"Recency - Monetary\": \"rfm*recency_monetary\"}, \n",
    "    \"Segmentation\":\n",
    "               {\"Customer Segmentation\": \"customer-segmentation*\",\n",
    "                \"Frequency Segmentation\": \"customer-segmentation*frequency_clusters\", \n",
    "                \"Monetary Segmentation\": \"customer-segmentation*monetary_clusters\", \n",
    "                \"Recency Segmentation\": \"customer-segmentation*recency_clusters\"}, \n",
    "    \"CLv Prediction\":\n",
    "               {\"Next Week CLV Predictions\": \"clv*daily_clv\", \n",
    "                \"CLV Predicted Nex Week Customers of Segments of Total Purchase Amounts\": \"clv*daily_clv\"},\n",
    "    \"Overall\":\n",
    "               {\"Customer Journey\": \"index*customer_journey\", \n",
    "                \"Total Number Customer Breakdown with Purchased Order Count\": \"clv*clvsegments_amount\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "asd = (\"adasd\", \"ssssss\")\n",
    "join(asd[0], asd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_str, date_str, prefix_date_str = [\"\"]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv = pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'clv_prediction'\")['data'])[0])\n",
    "clv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv.to_csv(\"/Users/mac/Downloads/es-7.10.0/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(\"*\" * 20) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = lambda dim: [print(\"*\" * 20) for i in range(3)] + [print(\"*\"*10, \" \", \" DIMENSION : \", dim, \"*\"*10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator(dim='location_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_WORKS_READABLE_FORM = {'clv_prediction': 'CLv Prediction',\n",
    "                            'abtest': 'A/B Test', 'funnel': 'Session Actions & Customers Actions Funnels',\n",
    "                            'cohort': 'Cohorts', 'rfm': 'RFM', 'stats': 'Descriptive Statistics',\n",
    "                            'products': 'Product Analytics', 'segmentation': 'Customer Segmentation',\n",
    "                            'anomaly': 'Anomaly Detection'\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suc_log_for_data_works = lambda x: \" {0} is created! Check {0} sections. \".format(\n",
    "            DATA_WORKS_READABLE_FORM[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: suc_log_for_data_works('clv_prediction'), suc_log_for_data_works('abtest')\n",
    "except: print(\"asdsddd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.0 % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'clv_prediction'\")['data'])[-1])['client'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_funnel_anomaly = pd.DataFrame(list(pd.DataFrame(res).query(\n",
    "    \"report_name == 'anomaly' and type == 'daily_funnel'\")['data'])[0])\n",
    "daily_funnel_anomaly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = round(min(daily_funnel_anomaly['anomaly_scores']) - 0.01, 2)\n",
    "max_value = round(max(daily_funnel_anomaly['anomaly_scores']) + 0.01, 2)\n",
    "daily_funnel_anomaly['outlier'] = daily_funnel_anomaly['outlier'].apply(lambda x: max_value + 0.01 if x == 1 else min_value - 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    \n",
    "    go.Scatter(\n",
    "        name=\"anomaly score (0 - 1)\",\n",
    "        x=daily_funnel_anomaly['daily'],\n",
    "        y=daily_funnel_anomaly['anomaly_scores'],\n",
    "        marker=dict(color=\"blue\")\n",
    "    ),\n",
    "    go.Bar(\n",
    "        name=\"outlier detection (0/1)\",\n",
    "        x=daily_funnel_anomaly['daily'],\n",
    "        y=daily_funnel_anomaly['outlier'],\n",
    "        marker=dict(color='#FECB52')\n",
    "    )\n",
    "    \n",
    "], layout=go.Layout(yaxis={\"range\": [\n",
    "                min_value, max_value]})\n",
    "                )\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_funnel_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Cohort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cohort_anomaly = pd.DataFrame(list(pd.DataFrame(res).query(\n",
    "    \"report_name == 'anomaly' and type == 'cohort_d'\")['data'])[0])\n",
    "cohort_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_anomaly_time_line = cohort_anomaly[['daily', 'anomaly_scores_from_d_to_1', 'outlier']\n",
    "              ].rename(columns={\"anomaly_scores_from_d_to_1\": \"Anomaly Score Download to First Order\"})\n",
    "cohort_anomaly_time_line.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    \n",
    "    go.Scatter(\n",
    "        name=\"anomaly score (0 - 1)\",\n",
    "        x=cohort_anomaly_time_line['daily'],\n",
    "        y=cohort_anomaly_time_line['Anomaly Score Download to First Order'],\n",
    "        marker=dict(color=\"blue\")\n",
    "    ),\n",
    "    go.Bar(\n",
    "        name=\"outlier detection (0/1)\",\n",
    "        x=cohort_anomaly_time_line['daily'],\n",
    "        y=cohort_anomaly_time_line['outlier'],\n",
    "        marker=dict(color='red')\n",
    "    )\n",
    "    \n",
    "]\n",
    "                )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_20_days = list(cohort_anomaly.query(\"daily > '2021-05-07'\")['daily'])\n",
    "last_20_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_20_days_cohort = cohort_anomaly.query(\"daily > '2021-05-07'\")[[str(i) for i in list(range(0, 5))]].transpose().reset_index()\n",
    "last_20_days_cohort.columns = ['days'] + last_20_days\n",
    "last_20_days_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlier_days = list(cohort_anomaly.query(\"outlier == 1\")['daily'])\n",
    "outlier_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for col in last_20_days_cohort.columns:\n",
    "    if col in outlier_days:\n",
    "        cols.append(col + '_outlier')\n",
    "    else:\n",
    "        cols.append(col)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "x = last_20_days_cohort['days']\n",
    "fig = go.Figure(data=[\n",
    "    \n",
    "    go.Scatter(\n",
    "        name=col,\n",
    "        x=x,\n",
    "        y=last_20_days_cohort[col],\n",
    "        marker=dict(color=\"red\") if col in outlier_days else dict(color=\"blue\")\n",
    "    ) for col in last_20_days\n",
    "    \n",
    "                    ]\n",
    "                )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_20_days_cohort.columns = cols\n",
    "last_20_days_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_orders_comparison = pd.DataFrame(list(pd.DataFrame(res).query(\n",
    "    \"report_name == 'anomaly' and type == 'daily_orders_comparison'\")['data'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Orders Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['diff_perc', 'daily']\n",
    "daily_orders_comparison_normal = daily_orders_comparison.query(\"anomalities == 'nomal decrease/increase'\")[columns]\n",
    "daily_orders_comparison_decrease = daily_orders_comparison.query(\"anomalities == 'significant decrease'\")[columns]\n",
    "daily_orders_comparison_increase = daily_orders_comparison.query(\"anomalities == 'significant increase'\")[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(metric):\n",
    "    if metric == 'nomal decrease/increase':\n",
    "        return 'yellow'\n",
    "    if metric == 'significant decrease':\n",
    "        return 'red'\n",
    "    if metric == 'significant increase':\n",
    "        return 'green'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naming_significance(metric):\n",
    "    if metric == 'nomal decrease/increase':\n",
    "        return 'no change'\n",
    "    if metric == 'significant decrease':\n",
    "        return 'decrease'\n",
    "    if metric == 'significant increase':\n",
    "        return 'increase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "x = daily_orders_comparison['daily']\n",
    "fig = go.Figure(data=[\n",
    "    \n",
    "    go.Bar(\n",
    "        name=col,\n",
    "        x=x,\n",
    "        y=df['diff_perc'],\n",
    "        \n",
    "    ) for df, col in [(daily_orders_comparison_normal, 'nomal decrease/increase'), \n",
    "                  (daily_orders_comparison_decrease, 'significant decrease'), (daily_orders_comparison_increase, 'significant increase')]\n",
    "    \n",
    "                    ]\n",
    "                )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLV Prediction Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_rfm_change = pd.DataFrame(list(pd.DataFrame(res).query(\"report_name == 'anomaly' and type == 'clv_prediction'\")['data'])[0])\n",
    "clv_rfm_change.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_decision = lambda x: 'no change' if x.split(\" \")[1] == 'decrease/increase' else x.split(\" \")[1]\n",
    "naming = lambda x1, x2: \"Frequency; {0}, Monetary ; {1}\".format(naming_decision(x1), naming_decision(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_rfm_change['naming'] = clv_rfm_change.apply(lambda row: naming(row['f_anomaly'], row['m_anomaly']), axis=1)\n",
    "clv_rfm_change.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = sorted(list(clv_rfm_change['naming'].unique()))\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=[go.Scatter(name=cluster,\n",
    "                                 x=clv_rfm_change.query(\"naming == @cluster\")['monetary_diff'], \n",
    "                                 y=clv_rfm_change.query(\"naming == @cluster\")['frequency_diff'], \n",
    "                                 mode='markers') for cluster in clusters]\n",
    "                )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([daily_orders_comparison_normal,daily_orders_comparison_decrease, \n",
    "           daily_orders_comparison_increase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_funnel_anomaly = daily_funnel_anomaly.rename(columns={\"anomaly_scores\":\"Anomaly Score Download to First Order\"})\n",
    "daily_funnel_anomaly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_orders_comparison_normal['anomalities'] = 'no change'\n",
    "daily_orders_comparison_decrease['anomalities'] = 'decrease'\n",
    "daily_orders_comparison_increase['anomalities'] = 'increase'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([daily_orders_comparison_normal,daily_orders_comparison_decrease, \n",
    "           daily_orders_comparison_increase]).to_csv(\"exploratory_analysis/sample_data/sample_data_dorders_anomaly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_funnel_anomaly.to_csv(\"exploratory_analysis/sample_data/sample_data_dfunnel_anomaly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_funnel_anomaly.to_csv(\"exploratory_analysis/sample_data/sample_data_dfunnel_anomaly.csv\", index=False)\n",
    "last_20_days_cohort.to_csv(\"exploratory_analysis/sample_data/sample_data_dcohort_anomaly.csv\", index=False)\n",
    "cohort_anomaly_time_line.to_csv(\"exploratory_analysis/sample_data/sample_data_dcohort_anomaly_2.csv\", index=False)\n",
    "pd.concat([daily_orders_comparison_normal,daily_orders_comparison_decrease, \n",
    "           daily_orders_comparison_increase]).to_csv(\"exploratory_analysis/sample_data/sample_data_dorders_anomaly.csv\", index=False)\n",
    "clv_rfm_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_rfm_change.to_csv(\"exploratory_analysis/sample_data/sample_data_clvrfm_anomaly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).query(\n",
    "    \"report_name == 'anomaly'  and type == 'daily_orders_comparison' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
